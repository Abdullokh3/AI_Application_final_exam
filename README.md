# AI_Application_

WEEK_10_LAB_ALL_PARTS: 

Deeper CNNs as well as Pretrained Models 
• VGGNet
• GoogLeNet
• ResNet
• Transfer Learning
• Data Augmentation as a Regularization Technique
• Mistakes made by CNNs
• Reducing parameters with Depthwise Separable Convolution
• Striking the right network design balance with EfficientNet


WEEK_11_LAB: TORCHVISION:
https://github.com/Abdullokh3/AI_Application_final_exam/blob/main/week_11/Week_11_.ipynb
<img width="1280" alt="Screenshot 2022-12-09 at 5 47 15 PM" src="https://user-images.githubusercontent.com/90837231/206662088-086ffbff-a442-4d76-9e6b-c39603354d90.png">
In week 11, datagenerators were saved as a file to the colab working directory. 
Read the data, then divide it into train and test sets. We also import pandas. 
to read sales data from book stores and plot it. 
Graph the dataset. 
Plot naive prediction. 
the data used to train and test be uniform. Custom layer that retrieves . 
RNN output's sole remaining time step. 
Create a RNN model and train examples. 
Make test examples for the optimizer, loss function, and so on. 
Make Dataset objects. Using standardized data, make an naive prediction. and
train the simulation.

WEEK_12_LAB: Simple LSTM-based model
https://github.com/Abdullokh3/AI_Application_final_exam/blob/main/week_12/Week_12.ipynb
<img width="1280" alt="Screenshot 2022-12-09 at 5 40 02 PM" src="https://user-images.githubusercontent.com/90837231/206660573-a7fb0be5-32d4-4c06-bc09-3128461b640e.png">
We train and collect data from the Jena-clinate in the lab during week 12. 
We first downloaded the necessary data from the Keras dataset and
os was imported and opened for reading. We used to plot the graph. 
Calculate the number of samples we'll use for with matplotlib. 
split up each data set, normalize the data, etc. 
creating datasets for testing, validation, and training. 
examining one of our datasets' results. 
calculating the common sense baseline MAE. 
developing and testing a densely connected model. 
In addition, results of the loss and val_loss were plotted. 
AND experimenting with a Simple LSTM-based model and a 1D convolution model.


WEEK_13_LAB1: TORCHVISION
https://github.com/Abdullokh3/AI_Application_final_exam/blob/main/week_13/Week_13_lab_1.ipynb
<img width="1280" alt="Screenshot 2022-12-09 at 5 29 54 PM" src="https://user-images.githubusercontent.com/90837231/206659042-26a5d079-ba26-487b-ba54-c8bab87a3f95.png">
WEEK_13_LAB2: TORCHVISION
https://github.com/Abdullokh3/AI_Application_final_exam/blob/main/week_13/Week_13_lab_2.ipynb
<img width="1280" alt="Screenshot 2022-12-09 at 5 30 31 PM" src="https://user-images.githubusercontent.com/90837231/206659098-17c8b2f4-6ddb-4355-ad88-994c393a69f3.png">
We practiced layers in week 13 using TextVectorization 
layer and the Vectorizer class. We visualized the vocabulary
useing get_vocabulary() and text_vectorization. Then we downloaded required data.
showing shapes as well as datatypes of the first batch, 
Process words in groups: single words, bag of words method. 
For (unigrams), a binary encoding is used. 
Using a text vectorization layer to pre-process our datasets. 
Train and test the binary unigram model. 
Build and test a binary bigram model. And training as well as testing of TF-IDF bigram models.

WEEK_14_LAB_PART1:
https://github.com/Abdullokh3/AI_Application_final_exam/blob/main/week_14/Week_14_part1_transformer.ipynb
In the first section of the week 14 lab, we learned about  Transformer architecture. 
To understand self-attention
the generalized self-attention -> query-key-value model. 
focus with multi heads. 
The Transformer encoder 
In order to get started, we download some files from IMDB and some Stanford.edu data. 
After that, we prepared the data by including the necessary libraries, including OS and Pathlib. 
Random, TensorFlow, and Shutil. Transformation encoder and data vectorization. 
Implementation as a subclassed Layer is what comes next. 
A transformer encoder for text classification was trained and evaluated. 
model of the Transformer that is encoder-based. The next stage involves using positional encoding to implement positional embedding and re-inject order information. 
To put it all together, a text-classification Transformer is required for an underclassified layer. 
Combining positional embedding and the Transformer encoder.



WEEK_14_LAB_PART2: https://github.com/Abdullokh3/AI_Application_final_exam/blob/main/week_14/Week_14_Lab_part2.ipynb
Part 2 of the Lab for Week 14 lessons will cover learning from sequence to sequence. 
We began by obtaining the necessary data files from Tensorflow. Next,
num_val_samples and num_train_Preparing are a few defined variables. 
Samples, train_pairs, value_pairs, and are some of the datasets for the translation tasks.  Vectorizing the English and Spanish text pairs is a step in the preparation process. 
We printed the dataset inputs for the translation task. 
Using RNNs to learn from sequence to sequence, a GRU-based encoder is created. 
Using an end-to-end model and a GRU-based decoder, we trained our recurrent neural network. 
Sequence to Sequence Model After that, we translated fresh sentences using our resources. 
using RNN for encoding and decoding. 
Sequence-to-sequence learning transformer. 
The TransformerDecoder and The TransformerEncoder first appeared in our definitions. 
A machine translation transformer is the conclusion. 
After training the sequence-to-sequence Transformer, 
Our Transformer model allows us to translate new sentences.
<img width="1280" alt="Screenshot 2022-12-09 at 5 11 20 PM" src="https://user-images.githubusercontent.com/90837231/206655597-8b5e92ed-c241-430b-867d-02ae97331605.png">
