# AI_Application_final_exam




WEEK_14: 
Part 2 of the Lab for Week 14 lessons will cover learning from sequence to sequence. 
We began by obtaining the necessary data files from Tensorflow. Next,
num_val_samples and num_train_Preparing are a few defined variables. 
Samples, train_pairs, value_pairs, and are some of the datasets for the translation tasks.  Vectorizing the English and Spanish text pairs is a step in the preparation process. 
We printed the dataset inputs for the translation task. 
- Using RNNs to learn from sequence to sequence, a GRU-based encoder is created. 
Using an end-to-end model and a GRU-based decoder, we trained our recurrent neural network. 
Sequence to Sequence Model After that, we translated fresh sentences using our resources. 
using RNN for encoding and decoding. 
- Sequence-to-sequence learning transformer. 
The TransformerDecoder and The TransformerEncoder first appeared in our definitions. 
A machine translation transformer is the conclusion. 
After training the sequence-to-sequence Transformer, 
Our Transformer model allows us to translate new sentences.
