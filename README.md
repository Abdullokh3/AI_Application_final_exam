# AI_Application_




WEEK_14_LAB_PART2: https://github.com/Abdullokh3/AI_Application_final_exam/blob/main/week_14/Week_14_Lab_part2.ipynb
Part 2 of the Lab for Week 14 lessons will cover learning from sequence to sequence. 
We began by obtaining the necessary data files from Tensorflow. Next,
num_val_samples and num_train_Preparing are a few defined variables. 
Samples, train_pairs, value_pairs, and are some of the datasets for the translation tasks.  Vectorizing the English and Spanish text pairs is a step in the preparation process. 
We printed the dataset inputs for the translation task. 
Using RNNs to learn from sequence to sequence, a GRU-based encoder is created. 
Using an end-to-end model and a GRU-based decoder, we trained our recurrent neural network. 
Sequence to Sequence Model After that, we translated fresh sentences using our resources. 
using RNN for encoding and decoding. 
Sequence-to-sequence learning transformer. 
The TransformerDecoder and The TransformerEncoder first appeared in our definitions. 
A machine translation transformer is the conclusion. 
After training the sequence-to-sequence Transformer, 
Our Transformer model allows us to translate new sentences.
<img width="1280" alt="Screenshot 2022-12-09 at 5 11 20 PM" src="https://user-images.githubusercontent.com/90837231/206655597-8b5e92ed-c241-430b-867d-02ae97331605.png">
