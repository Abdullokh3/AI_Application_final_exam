# AI_Application_

WEEK_13

We practiced layers in week 13 using TextVectorization 
layer and the Vectorizer class. We visualized the vocabulary
useing get_vocabulary() and text_vectorization. Then we downloaded required data.
showing shapes as well as datatypes of the first batch, 
Process words in groups: single words, bag of words method. 
For (unigrams), a binary encoding is used. 
Using a text vectorization layer to pre-process our datasets. 
Train and test the binary unigram model. 
Build and test a binary bigram model. And training as well as testing of TF-IDF bigram models.

WEEK_14_LAB_PART1:
https://github.com/Abdullokh3/AI_Application_final_exam/blob/main/week_14/Week_14_part1_transformer.ipynb
In the first section of the week 14 lab, we learned about  Transformer architecture. 
To understand self-attention
the generalized self-attention -> query-key-value model. 
focus with multi heads. 
The Transformer encoder 
In order to get started, we download some files from IMDB and some Stanford.edu data. 
After that, we prepared the data by including the necessary libraries, including OS and Pathlib. 
Random, TensorFlow, and Shutil. Transformation encoder and data vectorization. 
Implementation as a subclassed Layer is what comes next. 
A transformer encoder for text classification was trained and evaluated. 
model of the Transformer that is encoder-based. The next stage involves using positional encoding to implement positional embedding and re-inject order information. 
To put it all together, a text-classification Transformer is required for an underclassified layer. 
Combining positional embedding and the Transformer encoder.



WEEK_14_LAB_PART2: https://github.com/Abdullokh3/AI_Application_final_exam/blob/main/week_14/Week_14_Lab_part2.ipynb
Part 2 of the Lab for Week 14 lessons will cover learning from sequence to sequence. 
We began by obtaining the necessary data files from Tensorflow. Next,
num_val_samples and num_train_Preparing are a few defined variables. 
Samples, train_pairs, value_pairs, and are some of the datasets for the translation tasks.  Vectorizing the English and Spanish text pairs is a step in the preparation process. 
We printed the dataset inputs for the translation task. 
Using RNNs to learn from sequence to sequence, a GRU-based encoder is created. 
Using an end-to-end model and a GRU-based decoder, we trained our recurrent neural network. 
Sequence to Sequence Model After that, we translated fresh sentences using our resources. 
using RNN for encoding and decoding. 
Sequence-to-sequence learning transformer. 
The TransformerDecoder and The TransformerEncoder first appeared in our definitions. 
A machine translation transformer is the conclusion. 
After training the sequence-to-sequence Transformer, 
Our Transformer model allows us to translate new sentences.
<img width="1280" alt="Screenshot 2022-12-09 at 5 11 20 PM" src="https://user-images.githubusercontent.com/90837231/206655597-8b5e92ed-c241-430b-867d-02ae97331605.png">
