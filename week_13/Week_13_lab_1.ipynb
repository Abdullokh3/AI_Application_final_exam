{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTIMjQ8hzIv4+cVsbRqNQZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdullokh3/AI_Application_final_exam/blob/main/week_13/Week_13_lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diX43--yE3rU"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "\n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in dataset:\n",
        "      text = self.standardize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "    self.inverse_vocabulary = dict(\n",
        "      (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "  def encode(self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "  def decode(self, int_sequence) :\n",
        "    return \" \".join(\n",
        "    self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "\n",
        "dataset = [\n",
        "  \"I write, erase, rewrite\",\n",
        "  \"Erase again, and then\",\n",
        "  \"A poppy blooms.\",\n",
        "]\n",
        "\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write. rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print (encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qW8WW9TG4FK",
        "outputId": "4aefbaee-fea9-430c-ac74-e302dcb61a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LXJfrB_G98K",
        "outputId": "4718cb07-7fd0-4146-d61d-777a22f00092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "output_mode=\"int\",\n",
        ")"
      ],
      "metadata": {
        "id": "J1hbM5JCHAak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "  lowercase_string = tf.strings.lower(string_tensor)\n",
        "  return tf.strings.regex_replace(\n",
        "    lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "  return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "  output_mode=\"int\",\n",
        "  standardize=custom_standardization_fn,\n",
        "  split=custom_split_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "hjD1HaO6HD3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "  \"I write, erase, rewrite\",\n",
        "  \"Erase again, and then\",\n",
        "  \"A poppy blooms.\",\n",
        "]\n",
        "\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "PuQoY1X8HICX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_npmpoWgHKS9",
        "outputId": "dd4ce2db-ffe0-4c86-a30f-5b54bbdb1788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Dd33iNHM4K",
        "outputId": "9cec86b6-dae5-4e7c-edc7-5e6252fac9e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33epxUQAHP94",
        "outputId": "d7041995-a289-42c1-966f-1f85a43e722c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgj2AyYbHUwS",
        "outputId": "58175dbe-8470-4d89-b524-254697e8033f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  22.2M      0  0:00:03  0:00:03 --:--:-- 22.2M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "eaHld9kyHYz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7kPB12RHwcV",
        "outputId": "be56207f-0f30-4db0-c16f-1d2dd60fa302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category)\n",
        "  random.Random(1337) .shuffle(files)\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname,\n",
        "      val_dir / category / fname)"
      ],
      "metadata": {
        "id": "kId2tdPrHzZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "  \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "  \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "  \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP6AANfnH_vw",
        "outputId": "1a75201a-1abb-47a8-82c0-361ca475dea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs. dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR-T4TLhICt9",
        "outputId": "0284129c-1a25-4aae-8dc0-9cd25fee1dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"this movie had a fairly good sounding plot, but the paste was very slow... very slow indeed. even if someone thinks this is a cult classic, i think that there are a lot better films from that era to be watched.<br /><br />the cinematography is not excellent, but not the worst either. the sounds are OK. lighting OK.<br /><br />i still wouldn't recommend this to anyone else than maybe a film-student.<br /><br />the movie does not contain music, and the horses having sex don't make it a good one either. and the woman masturbating on the edge of the bed was plain stupid.<br /><br />no winnings here, skip this utter boredom. i've seen worse believe me, but this is just waste of time, and i don't get the good reviews here. especially the high ratings...\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "  max_tokens=20000,\n",
        "  output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "  lambda x, y: (text_vectorization(x), y),\n",
        "  num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "  lambda x, y: (text_vectorization(x), y),\n",
        "  num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "  lambda x, y: (text_vectorization(x), y),\n",
        "  num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "DPJZB6DuIK0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMve3mGUIOS_",
        "outputId": "b87a01ee-8e2c-4ac3-eb11-1906c40d82b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "  inputs = keras.Input(shape=(max_tokens,))\n",
        "\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "\n",
        "  model.compile(optimizer=\"rmsprop\",\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "WNeZ6Gt7ISVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model ()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "  keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                  save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "  validation_data=binary_1gram_val_ds.cache(),\n",
        "  epochs=10,\n",
        "  callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJvID_X1IVtC",
        "outputId": "94d97ae8-6f66-4dd3-be79-4b836c855805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 11s 13ms/step - loss: 0.4169 - accuracy: 0.8230 - val_loss: 0.3000 - val_accuracy: 0.8790\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2856 - accuracy: 0.8949 - val_loss: 0.2900 - val_accuracy: 0.8850\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2551 - accuracy: 0.9125 - val_loss: 0.3011 - val_accuracy: 0.8848\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2433 - accuracy: 0.9205 - val_loss: 0.3166 - val_accuracy: 0.8846\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2315 - accuracy: 0.9234 - val_loss: 0.3259 - val_accuracy: 0.8840\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2285 - accuracy: 0.9268 - val_loss: 0.3398 - val_accuracy: 0.8842\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2258 - accuracy: 0.9265 - val_loss: 0.3556 - val_accuracy: 0.8780\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2188 - accuracy: 0.9298 - val_loss: 0.3624 - val_accuracy: 0.8782\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2196 - accuracy: 0.9323 - val_loss: 0.3663 - val_accuracy: 0.8784\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2220 - accuracy: 0.9319 - val_loss: 0.3775 - val_accuracy: 0.8790\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.2891 - accuracy: 0.8894\n",
            "Test acc: 0.889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "  ngrams=2,\n",
        "  max_tokens=20000,\n",
        "  output_mode=\"multi_hot\",\n",
        ")"
      ],
      "metadata": {
        "id": "HIYae13zIYpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "  lambda x, y: (text_vectorization(x), y),\n",
        "  num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "  lambda x, y: (text_vectorization(x), y),\n",
        "  num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "  lambda x, y: (text_vectorization(x), y),\n",
        "  num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "  keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\" ,\n",
        "                                  save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "  validation_data=binary_2gram_val_ds.cache(),\n",
        "  epochs=10,\n",
        "  callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6oXNMNhIndB",
        "outputId": "b1915b68-1b9a-4168-df10-7040326e80ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.3916 - accuracy: 0.8357 - val_loss: 0.2821 - val_accuracy: 0.8872\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2464 - accuracy: 0.9152 - val_loss: 0.2823 - val_accuracy: 0.8898\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2130 - accuracy: 0.9321 - val_loss: 0.2946 - val_accuracy: 0.8922\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1962 - accuracy: 0.9413 - val_loss: 0.3144 - val_accuracy: 0.8942\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1886 - accuracy: 0.9451 - val_loss: 0.3260 - val_accuracy: 0.8918\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1821 - accuracy: 0.9469 - val_loss: 0.3367 - val_accuracy: 0.8892\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1873 - accuracy: 0.9500 - val_loss: 0.3438 - val_accuracy: 0.8898\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1777 - accuracy: 0.9498 - val_loss: 0.3615 - val_accuracy: 0.8892\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1777 - accuracy: 0.9521 - val_loss: 0.3681 - val_accuracy: 0.8880\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1806 - accuracy: 0.9519 - val_loss: 0.3732 - val_accuracy: 0.8846\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.2714 - accuracy: 0.8984\n",
            "Test acc: 0.898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "  ngrams=2,\n",
        "  max_tokens=20000,\n",
        "  output_mode=\"count\"\n",
        ")"
      ],
      "metadata": {
        "id": "ISVGGwY8Iqi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "  ngrams=2,\n",
        "  max_tokens=20000,\n",
        "  output_mode=\"tf_idf\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZsG2dp9pJc9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "  lambda x, y: (text_vectorization(x), y),\n",
        "  num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "  lambda x, y: (text_vectorization(x), y),\n",
        "  num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "  lambda x, y: (text_vectorization(x), y),\n",
        "  num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary ()\n",
        "callbacks = [\n",
        "  keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "  validation_data=tfidf_2gram_val_ds.cache(),\n",
        "  epochs=10,\n",
        "  callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8QDGpsfJe5B",
        "outputId": "5e9629bf-76f6-431b-8c98-537d9590c032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 13ms/step - loss: 0.4800 - accuracy: 0.7937 - val_loss: 0.3067 - val_accuracy: 0.8758\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3199 - accuracy: 0.8718 - val_loss: 0.3119 - val_accuracy: 0.8730\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2810 - accuracy: 0.8871 - val_loss: 0.3229 - val_accuracy: 0.8828\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2726 - accuracy: 0.8938 - val_loss: 0.3331 - val_accuracy: 0.8778\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2576 - accuracy: 0.8978 - val_loss: 0.3465 - val_accuracy: 0.8780\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2447 - accuracy: 0.9044 - val_loss: 0.3577 - val_accuracy: 0.8708\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2334 - accuracy: 0.9096 - val_loss: 0.3524 - val_accuracy: 0.8818\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2196 - accuracy: 0.9170 - val_loss: 0.3557 - val_accuracy: 0.8866\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2052 - accuracy: 0.9215 - val_loss: 0.3463 - val_accuracy: 0.8818\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2072 - accuracy: 0.9204 - val_loss: 0.3949 - val_accuracy: 0.8810\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.3100 - accuracy: 0.8765\n",
            "Test acc: 0.876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "2iI_ixjkJjeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "  [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndMBG1rkJ_cf",
        "outputId": "355531dc-49bb-43ef-e6a9-2a9538376335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88.83 percent positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uz1t3nyhKCY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}